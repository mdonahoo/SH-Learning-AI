# Game Server Configuration
# Update GAME_HOST with your Starship Horizons server IP address
GAME_HOST=192.168.68.56
GAME_PORT_API=1864
GAME_PORT_WS=1865
GAME_PORT_HTTPS=1866
GAME_PORT_WSS=1867

# ==========================================
# MULTI-BRIDGE CONFIGURATION
# ==========================================
# Unique identifier for this recording station/bridge
# Leave empty or remove for single-bridge deployments
# Examples: Bridge-Alpha, Bridge-Beta, Bridge-Charlie
# BRIDGE_ID=Bridge-Alpha

# Audio Configuration
AUDIO_DEVICE_INDEX=0
SAMPLE_RATE=44100
CHUNK_SIZE=1024

# Recording Settings
RECORDING_PATH=./data/recordings
LOG_LEVEL=INFO
SESSION_STORAGE_PATH=./data/sessions/

# Metrics Database
METRICS_DB_PATH=./data/metrics.db

# Development Settings
DEBUG=False
TEST_MODE=False

# ==========================================
# AUDIO TRANSCRIPTION CONFIGURATION
# ==========================================

# Enable audio capture during missions
ENABLE_AUDIO_CAPTURE=true

# Audio Device Configuration
# Use scripts/list_audio_devices.py to find your device index
AUDIO_INPUT_DEVICE=0
AUDIO_SAMPLE_RATE=16000
AUDIO_CHANNELS=1
AUDIO_CHUNK_MS=100

# Whisper Model Configuration
# Model sizes: tiny, base, small, medium, large-v3
# Recommended: base (fast, good accuracy for bridge audio)
WHISPER_MODEL_SIZE=base
WHISPER_DEVICE=cpu
WHISPER_COMPUTE_TYPE=int8
WHISPER_MODEL_PATH=./data/models/whisper/

# Voice Activity Detection (VAD)
# Energy threshold for normalized float32 audio (range: 0.0-1.0)
# Typical values: 0.05-0.10 depending on room noise
# Use scripts/test_vad_levels.py (if available) to calibrate for your environment
VAD_ENERGY_THRESHOLD=0.08
MIN_SPEECH_DURATION=0.3
MIN_SILENCE_DURATION=0.3  # Shorter for better multi-speaker segmentation

# Transcription Settings
TRANSCRIBE_REALTIME=true
TRANSCRIBE_LANGUAGE=en
MIN_TRANSCRIPTION_CONFIDENCE=0.5

# Speaker Diarization
ENABLE_SPEAKER_DIARIZATION=true
USE_NEURAL_DIARIZATION=true
# SIMILARITY threshold: How similar voices must be (0.0-1.0) to be considered same speaker
# Higher = stricter matching = more unique speakers detected
# Recommended: 0.75-0.85 for distinct voices, 0.65-0.75 for similar voices
SPEAKER_EMBEDDING_THRESHOLD=0.75
MIN_EXPECTED_SPEAKERS=1
MAX_EXPECTED_SPEAKERS=6
EXPECTED_BRIDGE_CREW=6
BRIDGE_ROLES=Captain,Helm,Tactical,Science,Engineering,Communications
# HUGGINGFACE_TOKEN=your_token_here  # Required for neural diarization (pyannote.audio)
# Accept licenses at: https://huggingface.co/pyannote/speaker-diarization-3.1 and https://huggingface.co/pyannote/embedding

# Engagement Analytics
ENABLE_ENGAGEMENT_METRICS=true
ENGAGEMENT_UPDATE_INTERVAL=30.0

# Data Retention
# Save raw audio segments (.wav files) for each detected speech segment
# Enables playback of mission recordings with synchronized audio
# Note: Audio files are only saved when speech is detected by VAD
SAVE_RAW_AUDIO=false
AUDIO_RETENTION_DAYS=0
TRANSCRIPT_RETENTION_DAYS=30

# Performance Tuning
MAX_SEGMENT_QUEUE_SIZE=100
TRANSCRIPTION_WORKERS=2

# ============================================
# LLM INTEGRATION (Ollama)
# ============================================

# Enable LLM-powered mission reports
# Set to 'false' to disable automatic report generation
ENABLE_LLM_REPORTS=true

# Ollama Server Configuration
# Local (devcontainer): Start with `ollama serve &` then pull a model
#   OLLAMA_HOST=http://localhost:11434
#   OLLAMA_MODEL=llama3.2          # Smaller, faster (~2GB)
#   OLLAMA_MODEL=qwen2.5:7b        # Good balance (~4GB)
# Remote (dedicated GPU server):
#   OLLAMA_HOST=http://192.168.x.x:11434
#   OLLAMA_MODEL=qwen2.5:14b-instruct  # Higher quality (~9GB)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2
OLLAMA_TIMEOUT=120

# Report Generation Style
# Options: entertaining, professional, technical, casual
# 'entertaining' - Humorous, engaging narratives with pop culture references
# 'professional' - Formal, metric-driven analysis
# 'technical' - Detailed technical breakdowns
# 'casual' - Friendly, conversational tone
LLM_REPORT_STYLE=entertaining
