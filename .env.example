# Game Server Configuration
# Update GAME_HOST with your Starship Horizons server IP address
GAME_HOST=192.168.68.56
GAME_PORT_API=1864
GAME_PORT_WS=1865
GAME_PORT_HTTPS=1866
GAME_PORT_WSS=1867

# ==========================================
# MULTI-BRIDGE CONFIGURATION
# ==========================================
# Unique identifier for this recording station/bridge
# Leave empty or remove for single-bridge deployments
# Examples: Bridge-Alpha, Bridge-Beta, Bridge-Charlie
# BRIDGE_ID=Bridge-Alpha

# Audio Configuration
AUDIO_DEVICE_INDEX=0
SAMPLE_RATE=44100
CHUNK_SIZE=1024

# Recording Settings
RECORDING_PATH=./data/recordings
LOG_LEVEL=INFO
SESSION_STORAGE_PATH=./data/sessions/

# Metrics Database
METRICS_DB_PATH=./data/metrics.db

# Development Settings
DEBUG=False
TEST_MODE=False

# ==========================================
# AUDIO TRANSCRIPTION CONFIGURATION
# ==========================================

# Enable audio capture during missions
ENABLE_AUDIO_CAPTURE=true

# Audio Device Configuration
# Use scripts/list_audio_devices.py to find your device index
AUDIO_INPUT_DEVICE=0
AUDIO_SAMPLE_RATE=16000
AUDIO_CHANNELS=1
AUDIO_CHUNK_MS=100

# Whisper Model Configuration
# Model sizes: tiny, base, small, medium, large-v3
# large-v3 provides best accuracy (YouTube-quality transcription)
# Performance on CPU: tiny ~0.5s, base ~2-3s, small ~5-8s, large-v3 ~35-70s per segment
WHISPER_MODEL_SIZE=large-v3
WHISPER_DEVICE=cpu
WHISPER_COMPUTE_TYPE=int8
WHISPER_MODEL_PATH=./data/models/whisper/

# Voice Activity Detection (VAD)
# Energy threshold for normalized float32 audio (range: 0.0-1.0)
# Typical values: 0.05-0.10 depending on room noise
# Use scripts/test_vad_levels.py (if available) to calibrate for your environment
VAD_ENERGY_THRESHOLD=0.08
MIN_SPEECH_DURATION=0.3
MIN_SILENCE_DURATION=0.3  # Shorter for better multi-speaker segmentation

# Transcription Settings
TRANSCRIBE_REALTIME=true
TRANSCRIBE_LANGUAGE=en
# Lower confidence threshold to retain more transcripts (0.3-0.5 recommended)
MIN_TRANSCRIPTION_CONFIDENCE=0.3

# Analysis Confidence Filter
# Segments below this confidence are excluded from pattern analysis
# (Seven Habits, communication quality, scorecards) to avoid analyzing garbled text
# Set to 0.0 to disable filtering and analyze all segments
# Recommended: 0.10-0.30 depending on audio quality
ANALYSIS_CONFIDENCE_THRESHOLD=0.10

# Speaker Diarization
ENABLE_SPEAKER_DIARIZATION=true
USE_NEURAL_DIARIZATION=true

# CPU-Optimized Diarization (for systems without GPU)
# Options: auto, true, false
# - auto: Use CPU mode automatically when no GPU detected (default)
# - true: Force CPU mode (uses resemblyzer - 5-10x faster than pyannote on CPU)
# - false: Always use neural (pyannote) even on CPU
# CPU mode accuracy is within 5-10% of neural mode but much faster on CPU
USE_CPU_DIARIZATION=auto

# CPU Speaker Threshold (only used when USE_CPU_DIARIZATION is active)
# This is the minimum similarity for voices to be considered the SAME speaker
# HIGHER values = stricter matching = MORE speakers detected
# LOWER values = looser matching = FEWER speakers detected
# Recommended: 0.82-0.88 (default: 0.85)
# If detecting too few speakers, try 0.88-0.92
# If detecting too many speakers, try 0.78-0.82
CPU_SPEAKER_THRESHOLD=0.85

# SIMILARITY threshold: How similar voices must be (0.0-1.0) to be considered same speaker
# Higher = stricter matching = more unique speakers detected
# Recommended: 0.75-0.85 for distinct voices, 0.65-0.75 for similar voices
SPEAKER_EMBEDDING_THRESHOLD=0.75
MIN_EXPECTED_SPEAKERS=1
MAX_EXPECTED_SPEAKERS=6
EXPECTED_BRIDGE_CREW=6
BRIDGE_ROLES=Captain,Helm,Tactical,Science,Engineering,Communications
# HUGGINGFACE_TOKEN=your_token_here  # Required for neural diarization (pyannote.audio)
# Accept licenses at: https://huggingface.co/pyannote/speaker-diarization-3.1 and https://huggingface.co/pyannote/embedding

# Chunk-Based Diarization for Long Audio
# Audio longer than CHUNK_THRESHOLD is processed in chunks to prevent speaker drift
# Speaker drift occurs when embeddings accumulate errors over long recordings,
# causing distinct voices to merge into fewer clusters
DIARIZATION_CHUNK_THRESHOLD=600   # Process in chunks if audio > 10 minutes
DIARIZATION_CHUNK_DURATION=300    # Each chunk is 5 minutes
DIARIZATION_CHUNK_OVERLAP=30      # 30 second overlap between chunks

# Engagement Analytics
ENABLE_ENGAGEMENT_METRICS=true
ENGAGEMENT_UPDATE_INTERVAL=30.0

# Data Retention
# Save raw audio segments (.wav files) for each detected speech segment
# Enables playback of mission recordings with synchronized audio
# Note: Audio files are only saved when speech is detected by VAD
SAVE_RAW_AUDIO=false
AUDIO_RETENTION_DAYS=0
TRANSCRIPT_RETENTION_DAYS=30

# Performance Tuning
# Large queue to handle long recordings (large-v3 model is slower than real-time)
MAX_SEGMENT_QUEUE_SIZE=1000
# More workers for parallel transcription
TRANSCRIPTION_WORKERS=4

# ============================================
# LLM INTEGRATION (Ollama)
# ============================================

# Enable LLM-powered mission reports
# Set to 'false' to disable automatic report generation
ENABLE_LLM_REPORTS=true

# Ollama Server Configuration
# Local (devcontainer): Start with `ollama serve &` then pull a model
#   OLLAMA_HOST=http://localhost:11434
#   OLLAMA_MODEL=llama3.2          # Smaller, faster (~2GB)
#   OLLAMA_MODEL=qwen2.5:7b        # Good balance (~4GB)
# Remote (dedicated GPU server):
#   OLLAMA_HOST=http://192.168.x.x:11434
#   OLLAMA_MODEL=qwen2.5:14b-instruct  # Higher quality (~9GB)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=qwen2.5:14b-instruct
# Timeout for LLM generation in seconds (narrative summaries can take 1-2 minutes)
OLLAMA_TIMEOUT=600

# Report Generation Style
# Options: entertaining, professional, technical, casual
# 'entertaining' - Humorous, engaging narratives with pop culture references
# 'professional' - Formal, metric-driven analysis
# 'technical' - Detailed technical breakdowns
# 'casual' - Friendly, conversational tone
LLM_REPORT_STYLE=entertaining

# ============================================
# TELEMETRY-AUDIO CORRELATION
# ============================================

# Enable telemetry-based role confidence boosting
# Correlates console actions with nearby voice segments to validate speaker roles
# This can boost role confidence from ~75-85% (keyword-only) to ~90-95%

# Time window for matching telemetry events with audio segments (milliseconds)
# Events within this window of an audio segment will be correlated
CORRELATION_WINDOW_MS=500

# Minimum confidence boost from telemetry correlation (0.0-1.0)
# Applied for matches at the edge of the correlation window
MIN_CONFIDENCE_BOOST=0.1

# Maximum confidence boost from telemetry correlation (0.0-1.0)
# Applied for exact time matches (0ms delta)
MAX_CONFIDENCE_BOOST=0.3

# ============================================
# WEB SERVER CONFIGURATION
# ============================================

# Server host and port
WEB_SERVER_HOST=0.0.0.0
WEB_SERVER_PORT=8000

# Maximum file upload size in MB (default: 2048 = 2GB)
# Increase for very long recordings
WEB_MAX_UPLOAD_MB=2048

# CORS allowed origins (comma-separated)
WEB_CORS_ORIGINS=*
