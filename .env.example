# Game Server Configuration
# Update GAME_HOST with your Starship Horizons server IP address
GAME_HOST=192.168.68.56
GAME_PORT_API=1864
GAME_PORT_WS=1865
GAME_PORT_HTTPS=1866
GAME_PORT_WSS=1867

# ==========================================
# MULTI-BRIDGE CONFIGURATION
# ==========================================
# Unique identifier for this recording station/bridge
# Leave empty or remove for single-bridge deployments
# Examples: Bridge-Alpha, Bridge-Beta, Bridge-Charlie
# BRIDGE_ID=Bridge-Alpha

# Audio Configuration
AUDIO_DEVICE_INDEX=0
SAMPLE_RATE=44100
CHUNK_SIZE=1024

# Recording Settings
RECORDING_PATH=./data/recordings
LOG_LEVEL=INFO
SESSION_STORAGE_PATH=./data/sessions/

# Metrics Database
METRICS_DB_PATH=./data/metrics.db

# Development Settings
DEBUG=False
TEST_MODE=False

# ==========================================
# AUDIO TRANSCRIPTION CONFIGURATION
# ==========================================

# Enable audio capture during missions
ENABLE_AUDIO_CAPTURE=true

# Audio Device Configuration
# Use scripts/list_audio_devices.py to find your device index
AUDIO_INPUT_DEVICE=0
AUDIO_SAMPLE_RATE=16000
AUDIO_CHANNELS=1
AUDIO_CHUNK_MS=100

# Whisper Model Configuration
# Model sizes: tiny, base, small, medium, large-v3
# large-v3 provides best accuracy (YouTube-quality transcription)
# Performance on CPU: tiny ~0.5s, base ~2-3s, small ~5-8s, large-v3 ~35-70s per segment
WHISPER_MODEL_SIZE=large-v3
WHISPER_DEVICE=cpu
WHISPER_COMPUTE_TYPE=int8
WHISPER_MODEL_PATH=./data/models/whisper/

# Whisper Hotwords (faster-whisper beam search biasing)
# Comma-separated domain terms to bias beam search toward.
# Unlike initial_prompt, hotwords have NO token limit.
# Leave empty to use the built-in Starship Horizons hotwords list.
# WHISPER_HOTWORDS=warp, Hedge, Craylor, Kralien, nuke, terahertz

# Voice Activity Detection (VAD)
# Energy threshold for normalized float32 audio (range: 0.0-1.0)
# Typical values: 0.05-0.10 depending on room noise
# Use scripts/test_vad_levels.py (if available) to calibrate for your environment
VAD_ENERGY_THRESHOLD=0.08
MIN_SPEECH_DURATION=0.3
MIN_SILENCE_DURATION=0.3  # Shorter for better multi-speaker segmentation

# Transcription Settings
TRANSCRIBE_REALTIME=true
TRANSCRIBE_LANGUAGE=en
# Lower confidence threshold to retain more transcripts (0.3-0.5 recommended)
MIN_TRANSCRIPTION_CONFIDENCE=0.3

# Analysis Confidence Filter
# Segments below this confidence are excluded from pattern analysis
# (Seven Habits, communication quality, scorecards) to avoid analyzing garbled text
# Set to 0.0 to disable filtering and analyze all segments
# Recommended: 0.10-0.30 depending on audio quality
ANALYSIS_CONFIDENCE_THRESHOLD=0.10

# Speaker Diarization
ENABLE_SPEAKER_DIARIZATION=true
USE_NEURAL_DIARIZATION=true

# CPU-Optimized Diarization (for systems without GPU)
# Options: auto, true, false
# - auto: Use CPU mode automatically when no GPU detected
# - true: Force CPU mode (uses resemblyzer - 5-10x faster than pyannote on CPU)
# - false: Always use neural (pyannote) even on CPU
# IMPORTANT: Set to 'false' to ensure consistent reports across all environments.
# 'auto' causes GPU systems to use pyannote and CPU systems to use resemblyzer,
# which are different algorithms producing different speaker clustering results.
USE_CPU_DIARIZATION=false

# CPU Speaker Threshold (only used when USE_CPU_DIARIZATION is active)
# This is the minimum similarity for voices to be considered the SAME speaker
# HIGHER values = stricter matching = MORE speakers detected
# LOWER values = looser matching = FEWER speakers detected
# Recommended: 0.60-0.70 (default: 0.60 for 6-crew bridges)
# If detecting too few speakers, try 0.70-0.80
# If detecting too many speakers, try 0.50-0.60
CPU_SPEAKER_THRESHOLD=0.60

# SIMILARITY threshold: How similar voices must be (0.0-1.0) to be considered same speaker
# Higher = stricter matching = more unique speakers detected
# Lower = more aggressive voice merging = fewer speakers detected
# Recommended: 0.60-0.70 for bridge crews (0.60 for 6 crew), 0.75-0.85 for similar voices/TTS
SPEAKER_EMBEDDING_THRESHOLD=0.60
MIN_EXPECTED_SPEAKERS=3
MAX_EXPECTED_SPEAKERS=8
EXPECTED_BRIDGE_CREW=6
BRIDGE_ROLES=Captain,Helm,Tactical,Science,Engineering,Communications
# Required for neural diarization (pyannote.audio)
# You MUST set this token and accept model licenses for neural diarization to work.
# Accept licenses at: https://huggingface.co/pyannote/speaker-diarization-3.1
#                 and: https://huggingface.co/pyannote/embedding
HUGGINGFACE_TOKEN=your_token_here

# Chunk-Based Diarization for Long Audio
# Audio longer than CHUNK_THRESHOLD is processed in chunks to prevent speaker drift
# Speaker drift occurs when embeddings accumulate errors over long recordings,
# causing distinct voices to merge into fewer clusters
DIARIZATION_CHUNK_THRESHOLD=900   # Process in chunks if audio > 15 minutes
DIARIZATION_CHUNK_DURATION=600    # Each chunk is 10 minutes
DIARIZATION_CHUNK_OVERLAP=30      # 30 second overlap between chunks

# Audio Preprocessing for Better Diarization
# Preprocessing improves voice embedding quality by reducing noise
DIARIZATION_PREPROCESS=true       # Enable audio preprocessing before diarization
# Noise reduction strength (0.0-1.0): higher = more aggressive noise removal
# 0.3 is conservative, 0.5 is moderate (good for multi-speaker bridge), 0.7+ may affect voice quality
NOISE_REDUCE_STRENGTH=0.5

# Sub-Segment Diarization (for splitting multi-speaker segments)
# Segments longer than this threshold are analyzed for multiple speakers
SUBSEGMENT_THRESHOLD=3.0           # Segments > 3s get sub-segmented
SUBSEGMENT_WINDOW=1.5              # Extract embedding every 1.5s
SUBSEGMENT_HOP=0.5                 # Hop 0.5s for fine-grained detection
SUBSEGMENT_SPEAKER_THRESHOLD=0.75  # Similarity threshold within segment

# Speaker Change Detection
# Minimum pause duration to consider as potential speaker boundary
MIN_PAUSE_DURATION=0.7             # 0.7s pause = potential speaker switch
# Minimum segment duration after splitting (prevents micro-segments)
MIN_SPLIT_SEGMENT_DURATION=2.5     # At least 2.5 seconds per speaker
# Minimum segments for a speaker to be "major" (smaller get merged)
MIN_SPEAKER_SEGMENTS=5             # Speakers with < 5 segments get consolidated

# Engagement Analytics
ENABLE_ENGAGEMENT_METRICS=true
ENGAGEMENT_UPDATE_INTERVAL=30.0

# Data Retention
# Save raw audio segments (.wav files) for each detected speech segment
# Enables playback of mission recordings with synchronized audio
# Note: Audio files are only saved when speech is detected by VAD
SAVE_RAW_AUDIO=false
AUDIO_RETENTION_DAYS=0
TRANSCRIPT_RETENTION_DAYS=30

# Performance Tuning
# Large queue to handle long recordings (large-v3 model is slower than real-time)
MAX_SEGMENT_QUEUE_SIZE=1000
# More workers for parallel transcription
TRANSCRIPTION_WORKERS=4

# ============================================
# LLM INTEGRATION (Ollama)
# ============================================

# Enable LLM-powered mission reports
# Set to 'false' to disable automatic report generation
ENABLE_LLM_REPORTS=true

# Ollama Server Configuration
# Local (devcontainer): Start with `ollama serve &` then pull a model
#   OLLAMA_HOST=http://localhost:11434
#   OLLAMA_MODEL=llama3.2              # Smaller, faster (~2GB)
#   OLLAMA_MODEL=qwen2.5:7b            # Good balance (~4GB)
# Remote (dedicated GPU server):
#   OLLAMA_HOST=http://192.168.x.x:11434
#   OLLAMA_MODEL=qwen2.5:14b-instruct  # Higher quality (~9GB)
# Production (dedicated GPU with 48GB+ VRAM):
#   OLLAMA_MODEL=qwen2.5:72b-instruct  # Best quality (~40GB VRAM, Q4)
#   OLLAMA_MODEL=llama3.3:70b          # Strong alternative (~40GB VRAM, Q4)
# Production (multi-GPU with 64GB+ VRAM):
#   OLLAMA_MODEL=qwen2.5:110b          # Maximum quality (~64GB VRAM, Q4)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=qwen2.5:14b-instruct
# Context window size in tokens (must be large enough for your prompts)
# qwen2.5 supports up to 131072; set to model max for full transcript context
OLLAMA_NUM_CTX=32768
# Number of parallel workers for LLM transcript cleanup batches
# Higher = faster cleanup but more concurrent Ollama requests
LLM_CLEANUP_WORKERS=3
# Timeout for LLM generation in seconds (narrative summaries can take longer for big missions)
OLLAMA_TIMEOUT=5400

# Report Generation Style
# Options: entertaining, professional, technical, casual
# 'entertaining' - Humorous, engaging narratives with pop culture references
# 'professional' - Formal, metric-driven analysis
# 'technical' - Detailed technical breakdowns
# 'casual' - Friendly, conversational tone
LLM_REPORT_STYLE=entertaining

# ============================================
# TELEMETRY-AUDIO CORRELATION
# ============================================

# Enable telemetry-based role confidence boosting
# Correlates console actions with nearby voice segments to validate speaker roles
# This can boost role confidence from ~75-85% (keyword-only) to ~90-95%

# Time window for matching telemetry events with audio segments (milliseconds)
# Events within this window of an audio segment will be correlated
CORRELATION_WINDOW_MS=500

# Minimum confidence boost from telemetry correlation (0.0-1.0)
# Applied for matches at the edge of the correlation window
MIN_CONFIDENCE_BOOST=0.1

# Maximum confidence boost from telemetry correlation (0.0-1.0)
# Applied for exact time matches (0ms delta)
MAX_CONFIDENCE_BOOST=0.3

# ============================================
# PARALLEL ANALYSIS PIPELINE
# ============================================
# Master toggle for GPU-aware parallel analysis
# When true: CPU metrics run in parallel, LLM calls run in parallel
# When false: All analysis steps run sequentially (original behavior)
ENABLE_PARALLEL_ANALYSIS=true

# Multi-GPU Whisper+diarization (requires 2+ CUDA GPUs)
# When true and 2+ GPUs: Whisper on GPU:0 and pyannote on GPU:1 simultaneously
ENABLE_PARALLEL_GPU=true

# Parallel CPU metrics (quality, scorecards, confidence, learning, habits, captain)
# These 6 independent analysis steps run simultaneously via ThreadPoolExecutor
ENABLE_PARALLEL_CPU=true

# Parallel LLM generation (narrative + story run simultaneously)
# Even if Ollama queues internally, this eliminates scheduling overhead
ENABLE_PARALLEL_LLM=true

# Number of CPU workers for parallel metrics (0 = auto: cpu_count - 1, max 6)
PARALLEL_CPU_WORKERS=0

# ============================================
# WEB SERVER CONFIGURATION
# ============================================

# Server host and port
WEB_SERVER_HOST=0.0.0.0
WEB_SERVER_PORT=8000

# Maximum file upload size in MB (default: 2048 = 2GB)
# Increase for very long recordings
WEB_MAX_UPLOAD_MB=150

# CORS allowed origins (comma-separated)
WEB_CORS_ORIGINS=*

# Disable audio file upload/download
# When true: Recording, uploading, and downloading audio files is disabled
# Users can still view transcripts and download reports
# Useful for deployments where audio storage is not allowed
DISABLE_AUDIO_FILES=false

# Read-only mode - disable all analysis and processing
# When true: Server only displays existing reports, no new analysis or regeneration
# Implies DISABLE_AUDIO_FILES=true
# Users can only browse and download existing reports
READ_ONLY_MODE=false

# ============================================
# AZURE APPLICATION INSIGHTS (Optional)
# ============================================
# Connection string from Azure Portal > Application Insights > Overview
# Leave empty to disable telemetry completely
APPINSIGHTS_CONNECTION_STRING=

# Sampling percentage (0-100). Lower values reduce cost. Default: 100
APPINSIGHTS_SAMPLING_PERCENTAGE=100

# Cloud role name for distributed tracing
APPINSIGHTS_CLOUD_ROLE=sh-learning-ai
